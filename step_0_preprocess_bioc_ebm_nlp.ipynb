{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91149b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bioc import biocxml\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from typing import Any, List\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f49d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_INPUT_FILE = 'data/bioc/input/ebm_nlp_2_00_sample_ssplit.xml'\n",
    "SAMPLE_OUTPUT_PATH = 'data/bioc/json/sample'\n",
    "SAMPLE_SPAN_OUTPUT_PATH = 'data/bioc/span/sample'\n",
    "\n",
    "INPUT_FILE = 'data/bioc/input/ebm_nlp_2_00_ssplit.xml'\n",
    "# OUTPUT_PATH = 'data/bioc/json/no_overlap_training'\n",
    "# OUTPUT_PATH = 'data/bioc/json/sample'\n",
    "OUTPUT_PATH = 'data/bioc/json'\n",
    "\n",
    "class DatasetSplit(Enum):\n",
    "    train = 0\n",
    "    validation = 1\n",
    "    test = 2\n",
    "\n",
    "class PicoType(Enum):\n",
    "    PARTICIPANTS = 4\n",
    "    INTERVENTIONS = 2\n",
    "    OUTCOMES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d0538",
   "metadata": {},
   "source": [
    "### Format EBM-NLP dataset for Sequence Labeling.\n",
    "\n",
    "See https://github.com/gzhang64/EBM-NLP for detailed description of the dataset. To conveniently fine-tune transformer models for sequence labeling, we convert each sentence in an annotated PubMed abstract to a JSON object with the following format:\n",
    "\n",
    "Column Name | Data Type | Description\n",
    ":--- | :--- | :---\n",
    "pmid | String | PubMed ID of the annotated abstract.\n",
    "tokens | List(String) | The list of tokens in a sentence from an annotated abstract.\n",
    "interventions | List(String) | Intervention spanning tags, e.g. \"B-interventions\", \"I-interventions\", \"O\".\n",
    "outcomes | List(String) | Outcome tags corresponding to each token.\n",
    "participants | List(String) | Participant tags corresponding to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd73c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pmid(folder):\n",
    "    return set([f.rstrip('.AGGREGATED.ann') for f in os.listdir(folder) if f.endswith('.AGGREGATED.ann')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a14bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTICIPANTS: train 4792, test 189\n",
      "INTERVENTIONS: train 4782, test 188\n",
      "OUTCOMES: train 4670, test 190\n"
     ]
    }
   ],
   "source": [
    "participants_train = load_pmid('data/raw/ebm_nlp_2_00/annotations/aggregated/starting_spans/participants/train')\n",
    "interventions_train = load_pmid('data/raw/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/train')\n",
    "outcomes_train = load_pmid('data/raw/ebm_nlp_2_00/annotations/aggregated/starting_spans/outcomes/train')\n",
    "\n",
    "participants_test = load_pmid('data/raw/ebm_nlp_2_00/annotations/aggregated/starting_spans/participants/test/gold')\n",
    "interventions_test = load_pmid('data/raw/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/test/gold')\n",
    "outcomes_test = load_pmid('data/raw/ebm_nlp_2_00/annotations/aggregated/starting_spans/outcomes/test/gold')\n",
    "\n",
    "print(f'PARTICIPANTS: train {len(participants_train)}, test {len(participants_test)}')\n",
    "print(f'INTERVENTIONS: train {len(interventions_train)}, test {len(interventions_test)}')\n",
    "print(f'OUTCOMES: train {len(outcomes_train)}, test {len(outcomes_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a618c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EBM-NLP dataset as a BioC collection.\n",
    "with open(INPUT_FILE, 'r', encoding='UTF-8') as fp:\n",
    "    collection = biocxml.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7dda173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 4993\n",
      "Train size: 4566\n",
      "Validation size: 241\n",
      "Test size: 186\n"
     ]
    }
   ],
   "source": [
    "all_pmid = set([document.id for document in collection.documents])\n",
    "test_pmid = participants_test.intersection(interventions_test, outcomes_test)\n",
    "train_pmid_list = list(all_pmid - test_pmid)\n",
    "train_validation_boundary = int(0.95 * len(train_pmid_list))\n",
    "train_pmid = set(train_pmid_list[:train_validation_boundary])\n",
    "validation_pmid = set(train_pmid_list[train_validation_boundary:])\n",
    "\n",
    "print(f'Total size: {len(collection.documents)}')\n",
    "print(f'Train size: {len(collection.documents) - len(validation_pmid)- len(test_pmid)}')\n",
    "print(f'Validation size: {len(validation_pmid)}')\n",
    "print(f'Test size: {len(test_pmid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1028cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of extracted tokens and labels (Interventions):\n",
      "[\tO\tO\tO\n",
      "Triple\tO\tO\tO\n",
      "therapy\tO\tO\tO\n",
      "regimens\tO\tO\tO\n",
      "involving\tO\tO\tO\n",
      "H2\tO\tB-INTERVENTIONS\tO\n",
      "blockaders\tO\tI-INTERVENTIONS\tO\n",
      "for\tO\tO\tO\n",
      "therapy\tO\tO\tO\n",
      "of\tO\tO\tO\n",
      "Helicobacter\tO\tO\tB-OUTCOMES\n",
      "pylori\tO\tO\tI-OUTCOMES\n",
      "infections\tO\tO\tI-OUTCOMES\n",
      "]\tO\tO\tI-OUTCOMES\n",
      ".\tO\tO\tO\n",
      "Has overlapping span:  False\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Extracts PIO labels from a BioC Sentence object.\n",
    "\n",
    "Parameters:\n",
    "    sentence (BioC sentence): a BioC sentence object that contains tokens and labels.\n",
    "    pico_type (enum PicoType): one of PicoType.{INTERVENTIONS, OUTCOMES, PARTICIPANTS}\n",
    "\n",
    "Returns:\n",
    "    A list of PICO tags corresponding to each token or None if the labels are not present.\n",
    "'''\n",
    "def extract_pio_labels(sentence: Any, pico_type: Any) -> List[Any]:\n",
    "    label_name = 'starting_spans/{}'.format(pico_type.name.lower())\n",
    "    if not sentence.annotations:\n",
    "        return None\n",
    "    if label_name not in sentence.annotations[0].infons:\n",
    "        return [None for _ in sentence.annotations]\n",
    "    raw_labels = [a.infons[label_name] for a in sentence.annotations]\n",
    "    b_tag = 'B-{}'.format(pico_type.name)\n",
    "    i_tag = 'I-{}'.format(pico_type.name)\n",
    "    o_tag = 'O'\n",
    "    labels = []\n",
    "    for i, raw_label in enumerate(raw_labels):\n",
    "        if raw_label == '0':\n",
    "            labels.append(o_tag)\n",
    "        elif i == 0:\n",
    "            labels.append(b_tag)\n",
    "        else:\n",
    "            labels.append(i_tag if raw_labels[i - 1] == '1' else b_tag)\n",
    "    return labels\n",
    "\n",
    "'''Extract tokens from a BioC Sentence object.'''\n",
    "def extract_tokens(sentence: Any) -> List[str]:\n",
    "    return [a.text for a in sentence.annotations]\n",
    "\n",
    "'''Checks if PIO labels have overlapping tokens.'''\n",
    "def check_overlapping(sentence: Any) -> bool:\n",
    "    def extract_raw_span_labels(sentence, pico_type):\n",
    "        label_name = 'starting_spans/{}'.format(pico_type.name.lower())\n",
    "        if not sentence.annotations or label_name not in sentence.annotations[0].infons:\n",
    "            return [0 for _ in sentence.annotations]\n",
    "        return [int(a.infons[label_name]) for a in sentence.annotations]\n",
    "    \n",
    "    p_labels = extract_raw_span_labels(sentence, PicoType.PARTICIPANTS)\n",
    "    i_labels = extract_raw_span_labels(sentence, PicoType.INTERVENTIONS)\n",
    "    o_labels = extract_raw_span_labels(sentence, PicoType.OUTCOMES)\n",
    "    \n",
    "    all_labels = [p + i + o for p, i, o in zip(p_labels, i_labels, o_labels)]\n",
    "    return max(all_labels) > 1\n",
    "\n",
    "print('Example of extracted tokens and labels (Interventions):')\n",
    "example_sentence = collection.documents[0].passages[0].sentences[0]\n",
    "example_tokens = extract_tokens(example_sentence)\n",
    "example_participant_labels = extract_pio_labels(example_sentence,PicoType.PARTICIPANTS)\n",
    "example_intervention_labels = extract_pio_labels(example_sentence,PicoType.INTERVENTIONS)\n",
    "example_outcome_labels = extract_pio_labels(example_sentence,PicoType.OUTCOMES)\n",
    "\n",
    "assert len(example_tokens) == len(example_participant_labels)\n",
    "assert len(example_tokens) == len(example_intervention_labels)\n",
    "assert len(example_tokens) == len(example_outcome_labels)\n",
    "\n",
    "for token, p, i, o in zip(\n",
    "    example_tokens,\n",
    "    example_participant_labels,\n",
    "    example_intervention_labels,\n",
    "    example_outcome_labels):\n",
    "    print('{}\\t{}\\t{}\\t{}'.format(token, p, i, o))\n",
    "    \n",
    "print('Has overlapping span: ', check_overlapping(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac1523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates a training/validation/test dataset.\n",
    "\n",
    "Creates a dataset and saves it on disk. Each line of the output file is a JSON dump.\n",
    "The output file itself is not a valid JSON object.\n",
    "\n",
    "Parameters:\n",
    "    collection (BioC Collection): a BioC collection objects containing raw data.\n",
    "    dataset_split (enum DatasetSplit): one of DatasetSplit.{train, validation, test}.\n",
    "    output_path (string): directory where json files are stored.\n",
    "    include_overlapping_spans (bool): whether to include overlapping spans, default True\n",
    "    only_overlapping_spans (bool): whether to include only overlapping spans, default False\n",
    "'''\n",
    "def create_dataset(\n",
    "    collection: Any,\n",
    "    dataset_split: Any,\n",
    "    output_path: str,\n",
    "    include_overlapping_spans: bool = True,\n",
    "    only_overlapping_spans: bool = False,\n",
    "):\n",
    "    if dataset_split == DatasetSplit.train:\n",
    "        target_pmid_set = train_pmid\n",
    "    elif dataset_split == DatasetSplit.validation:\n",
    "        target_pmid_set = validation_pmid\n",
    "    else:\n",
    "        target_pmid_set = test_pmid\n",
    "        \n",
    "    output_file = os.path.join(output_path, '{}.json'.format(dataset_split.name))\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    overlap_sentence_count = 0\n",
    "    total_sentence_count = 0\n",
    "    \n",
    "    with open(output_file, 'w+') as fout:\n",
    "        for document in collection.documents:\n",
    "            pmid = document.id\n",
    "            if pmid not in target_pmid_set:\n",
    "                continue\n",
    "            for passage in document.passages:\n",
    "                for sentence in passage.sentences:\n",
    "                    data = {}\n",
    "                    data['pmid'] = pmid\n",
    "                    data['tokens'] = extract_tokens(sentence)\n",
    "                    participants = extract_pio_labels(sentence, PicoType.PARTICIPANTS)\n",
    "                    interventions = extract_pio_labels(sentence, PicoType.INTERVENTIONS)\n",
    "                    outcomes = extract_pio_labels(sentence, PicoType.OUTCOMES)\n",
    "                    \n",
    "                    if not participants or not interventions or not outcomes:\n",
    "                        warnings.warn('Empty annotations in abstract {}.'.format(pmid))\n",
    "                        continue\n",
    "                    \n",
    "                    total_sentence_count += 1\n",
    "                    if check_overlapping(sentence):\n",
    "                        overlap_sentence_count += 1\n",
    "                        if not include_overlapping_spans:\n",
    "                            continue\n",
    "                    elif only_overlapping_spans:\n",
    "                        continue\n",
    "                    \n",
    "                    \n",
    "                    labels = []\n",
    "                    for p, i, o in zip(participants, interventions, outcomes):\n",
    "                        label = 0\n",
    "                        if p not in ['O', None]:\n",
    "                            label = label | PicoType.PARTICIPANTS.value\n",
    "                        if i not in ['O', None]:\n",
    "                            label = label | PicoType.INTERVENTIONS.value\n",
    "                        if o not in ['O', None]:\n",
    "                            label = label | PicoType.OUTCOMES.value\n",
    "                        labels.append(label)\n",
    "                    data['labels'] = labels\n",
    "                    fout.write('{}\\n'.format(json.dumps(data)))\n",
    "    print('{}: {}/{} sentences have overlapping spans.'.format(\n",
    "        dataset_split.name, overlap_sentence_count, total_sentence_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9033f497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4490/48960 sentences have overlapping spans.\n",
      "validation: 231/2542 sentences have overlapping spans.\n",
      "test: 115/2042 sentences have overlapping spans.\n"
     ]
    }
   ],
   "source": [
    "create_dataset(collection, DatasetSplit.train, OUTPUT_PATH)\n",
    "create_dataset(collection, DatasetSplit.validation, OUTPUT_PATH)\n",
    "create_dataset(collection, DatasetSplit.test, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c154a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4490/48960 sentences have overlapping spans.\n",
      "test: 115/2042 sentences have overlapping spans.\n"
     ]
    }
   ],
   "source": [
    "# data with no overlapping spans\n",
    "create_dataset(collection, DatasetSplit.train, os.path.join(OUTPUT_PATH, 'no_overlap'), include_overlapping_spans=False)\n",
    "create_dataset(collection, DatasetSplit.test, os.path.join(OUTPUT_PATH, 'no_overlap'), include_overlapping_spans=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a17dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 115/2042 sentences have overlapping spans.\n"
     ]
    }
   ],
   "source": [
    "# data with overlapping spans only\n",
    "create_dataset(collection, DatasetSplit.test, os.path.join(OUTPUT_PATH, 'overlap_only'), only_overlapping_spans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57dc562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, start, length):\n",
    "        self.start = start\n",
    "        self.length = length\n",
    "        self.end = start + length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Span(start:{self.start}, length:{self.length})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.start, int(self.length)) == (other.start, int(other.length))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.start, int(self.length)))\n",
    "\n",
    "def extract_pico_spans(sentence, pico_type):\n",
    "    label_name = 'starting_spans/{}'.format(pico_type.name.lower())\n",
    "    if not sentence.annotations:\n",
    "        return []\n",
    "    if label_name not in sentence.annotations[0].infons:\n",
    "        return []\n",
    "    \n",
    "    labels = [int(a.infons[label_name]) for a in sentence.annotations]\n",
    "    span_start = [0 for l in labels]\n",
    "    span_length = [0.0 for l in labels]\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label > 0:\n",
    "            if i==0 or labels[i-1] <= 0:\n",
    "                span_start[i] = 1\n",
    "                start = i\n",
    "            span_length[start] += 1\n",
    "            \n",
    "    spans = []\n",
    "    for i in range(len(span_start)):\n",
    "        if span_start[i]:\n",
    "            s = Span(start=i, length=span_length[i])\n",
    "            spans.append(s)\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e49d76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_span_util(span_a, span_b):\n",
    "    if span_a.start > span_b.start:\n",
    "        span_a, span_b = span_b, span_a\n",
    "    spans = []\n",
    "    if span_b.start >= span_a.end:\n",
    "        spans.append(Span(span_a.start, span_b.end - span_a.start))\n",
    "    else:\n",
    "        if span_a.start != span_b.start and span_a.start != span_b.end:\n",
    "            spans.append(Span(span_b.start, span_a.end - span_b.start))\n",
    "            spans.append(Span(span_a.start, span_b.end - span_a.start))\n",
    "    return spans\n",
    "        \n",
    "\n",
    "def synthesize_spans(span_list_a, span_list_b):\n",
    "    spans = []\n",
    "    for a in span_list_a:\n",
    "        for b in span_list_b:\n",
    "            spans += synthesize_span_util(a, b)\n",
    "    return spans\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4a5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates a training/validation/test dataset for span classification.\n",
    "\n",
    "Parameters:\n",
    "    collection (BioC Collection): a BioC collection objects containing raw data.\n",
    "    dataset_split (enum DatasetSplit): one of DatasetSplit.{train, validation, test}.\n",
    "    output_path (string): directory where json files are stored.\n",
    "'''\n",
    "def create_span_clf_dataset(collection: Any, dataset_split: Any, output_path: str):\n",
    "    if dataset_split == DatasetSplit.train:\n",
    "        target_pmid_set = train_pmid\n",
    "    elif dataset_split == DatasetSplit.validation:\n",
    "        target_pmid_set = validation_pmid\n",
    "    else:\n",
    "        target_pmid_set = test_pmid\n",
    "        \n",
    "    output_file = os.path.join(output_path, '{}_span_clf.json'.format(dataset_split.name))\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    count = defaultdict(lambda: 0)\n",
    "    with open(output_file, 'w+') as fout:\n",
    "        for document in collection.documents:\n",
    "            pmid = document.id\n",
    "            if pmid not in target_pmid_set:\n",
    "                continue\n",
    "            for passage in document.passages:\n",
    "                for sentence in passage.sentences:\n",
    "                    tokens = extract_tokens(sentence)\n",
    "                    # PICO spans\n",
    "                    for pico_type in list(PicoType):\n",
    "                        label = pico_type.name\n",
    "                        spans = extract_pico_spans(sentence, pico_type)\n",
    "                        if pico_type == PicoType.PARTICIPANTS:\n",
    "                            participants_spans = spans\n",
    "                        if pico_type == PicoType.INTERVENTIONS:\n",
    "                            interventions_spans = spans\n",
    "                        if pico_type == PicoType.OUTCOMES:\n",
    "                            outcomes_spans = spans\n",
    "                        for span in spans:\n",
    "                            data = {}\n",
    "                            data['pmid'] = pmid\n",
    "                            start, end = int(span.start), int(span.start + span.length)\n",
    "                            data['tokens'] = tokens[start:end]\n",
    "                            data['PARTICIPANTS'] = False\n",
    "                            data['INTERVENTIONS'] = False\n",
    "                            data['OUTCOMES'] = False\n",
    "                            data[pico_type.name] = True\n",
    "                            fout.write('{}\\n'.format(json.dumps(data)))\n",
    "                            count[label] += 1\n",
    "                    \n",
    "                    # synthesize spans using boundaries\n",
    "                    synthesized_spans = synthesize_spans(\n",
    "                        participants_spans, interventions_spans\n",
    "                    ) + synthesize_spans(\n",
    "                        participants_spans, outcomes_spans\n",
    "                    ) + synthesize_spans(\n",
    "                        interventions_spans, outcomes_spans\n",
    "                    )\n",
    "                    synthesized_spans = list(set(synthesized_spans))\n",
    "                    random.shuffle(synthesized_spans)\n",
    "                    sample_limit = 1\n",
    "                    for span in synthesized_spans[:sample_limit]:\n",
    "                        if (span not in participants_spans \n",
    "                            and span not in interventions_spans \n",
    "                            and span not in outcomes_spans):\n",
    "                            data = {}\n",
    "                            data['pmid'] = pmid\n",
    "                            start, end = int(span.start), int(span.start + span.length)\n",
    "                            data['tokens'] = tokens[start:end]\n",
    "                            data['PARTICIPANTS'] = False\n",
    "                            data['INTERVENTIONS'] = False\n",
    "                            data['OUTCOMES'] = False\n",
    "                            fout.write('{}\\n'.format(json.dumps(data)))\n",
    "                            count['SYNTHESIZED'] += 1\n",
    "                    \n",
    "                    # randomly sampled spans\n",
    "#                     if not synthesized_spans:\n",
    "#                         boundary = [\n",
    "#                             random.randint(0, len(tokens)-1),\n",
    "#                             random.randint(0, len(tokens)-1),\n",
    "#                         ]\n",
    "#                         data = {}\n",
    "#                         data['pmid'] = pmid\n",
    "#                         data['tokens'] = tokens[min(boundary) : max(boundary)]\n",
    "#                         data['PARTICIPANTS'] = False\n",
    "#                         data['INTERVENTIONS'] = False\n",
    "#                         data['OUTCOMES'] = False\n",
    "#                         fout.write('{}\\n'.format(json.dumps(data)))\n",
    "#                         count['RANDOM'] += 1\n",
    "                    \n",
    "    print('{}： {}'.format(dataset_split.name, ', '.join([f'{k}: {count[k]}' for k in count])))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f42f03f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train： INTERVENTIONS: 31177, OUTCOMES: 31844, PARTICIPANTS: 17011\n",
      "validation： PARTICIPANTS: 941, INTERVENTIONS: 1682, OUTCOMES: 1710\n",
      "test： PARTICIPANTS: 643, INTERVENTIONS: 1726, OUTCOMES: 1833\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "create_span_clf_dataset(collection, DatasetSplit.train, OUTPUT_PATH)\n",
    "create_span_clf_dataset(collection, DatasetSplit.validation, OUTPUT_PATH)\n",
    "create_span_clf_dataset(collection, DatasetSplit.test, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fbb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
