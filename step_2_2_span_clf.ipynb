{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce50e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import load_dataset, Sequence, ClassLabel\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3d9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'microsoft/BiomedNLP-PubMedBERT-large-uncased-abstract'\n",
    "SPAN_CLF_MODEL_PATH = 'pico_span/span_clf/span-clf-PICO_NER-ebm_nlp_bioc-2023_06_02_05_59_04_EDT/checkpoint-3020'\n",
    "# SPAN_CLF_MODEL_PATH = 'pico_span/span_clf/span-clf-PICO_NER-ebm_nlp_bioc-entity_only'\n",
    "INPUT_FOLDER = 'data/bioc/json/step_1_boundary_pred'\n",
    "OUTPUT_PATH = 'data/bioc/json/step_2_span_clf'\n",
    "\n",
    "class DatasetSplit(Enum):\n",
    "    train = 0\n",
    "    validation = 1\n",
    "    test = 2\n",
    "    \n",
    "class PicoType(Enum):\n",
    "    PARTICIPANTS = 4\n",
    "    INTERVENTIONS = 2\n",
    "    OUTCOMES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd7aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICO_CLASSES = [\n",
    "    'PARTICIPANTS', 'INTERVENTIONS', 'OUTCOMES',\n",
    "]\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(PICO_CLASSES)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5537106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/gzhang/.cache/huggingface/datasets/json/default-59f01fb16cb7545b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8349b1266d945efbcd4da33f0ac4c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebm_nlp = load_dataset(\n",
    "    'json',\n",
    "    data_files = {\n",
    "        'train': os.path.join(INPUT_FOLDER, 'test_boundary_pred.json'), # not used\n",
    "        'validation': os.path.join(INPUT_FOLDER, 'test_boundary_pred.json'),\n",
    "        'test': os.path.join(INPUT_FOLDER, 'test_boundary_pred.json')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81b705c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'tokens', 'original_labels', 'boundary_pred', 'start_confidence', 'end_confidence'],\n",
       "        num_rows: 2042\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['pmid', 'tokens', 'original_labels', 'boundary_pred', 'start_confidence', 'end_confidence'],\n",
       "        num_rows: 2042\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'tokens', 'original_labels', 'boundary_pred', 'start_confidence', 'end_confidence'],\n",
       "        num_rows: 2042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebm_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d06507b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(SPAN_CLF_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "761be67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_span_boundaries(boundary_pred):\n",
    "    starts = [i for i, p in enumerate(boundary_pred) if p & 1]\n",
    "    ends = [i for i, p in enumerate(boundary_pred) if p & 2]\n",
    "    candidates = []\n",
    "    for s in starts:\n",
    "        for e in ends:\n",
    "            if s<=e:\n",
    "                candidates.append((s, e))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd57d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, start, length):\n",
    "        self.start = start\n",
    "        self.length = length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Span(start={self.start}, length={self.length})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.start, int(self.length)) == (other.start, int(other.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed9402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_span_iou(a, b):\n",
    "    a_start, a_end = a.start, a.start + a.length\n",
    "    b_start, b_end = b.start, b.start + b.length\n",
    "    u = max(a_end, b_end) - min(a_start, b_start)\n",
    "    if u == 0.0:\n",
    "        return 0.0\n",
    "    i = 0.0\n",
    "    if a_start <= b_end and b_start <= a_end:\n",
    "        i = min(a_end, b_end) - max(a_start, b_start)\n",
    "    return i/u\n",
    "\n",
    "def nms_span(spans, span_class, confidence, iou_threshold=0.0):\n",
    "    keep = [True for _ in spans]\n",
    "    for i in range(len(spans) - 1):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "        for j in range(i+1, len(spans)):\n",
    "            if not keep[j] or span_class[i] != span_class[j]:\n",
    "                continue\n",
    "            iou = text_span_iou(spans[i], spans[j])\n",
    "            if iou > iou_threshold:\n",
    "                #  if confidence[i] < confidence[j]:\n",
    "                if spans[i].length > spans[j].length:\n",
    "                    keep[i] = False\n",
    "                else:\n",
    "                    keep[j] = False\n",
    "        \n",
    "    return (\n",
    "        [s for s, k in zip(spans, keep) if k],\n",
    "        [c for c, k in zip(span_class, keep) if k],\n",
    "        [c for c, k in zip(confidence, keep) if k],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1caabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pico_elements_util(tokens, boundary_pred, tokenizer, model, threshold=0.5):\n",
    "    candidate_spans = extract_span_boundaries(boundary_pred)\n",
    "    spans = []\n",
    "    pico_class = []\n",
    "    confidence = []\n",
    "    for span in candidate_spans:\n",
    "        start, end = span\n",
    "        content = tokens[start: end+1]\n",
    "        x = tokenizer(content, padding=True, return_tensors='pt', is_split_into_words=True)\n",
    "        y = model(**x)\n",
    "        probability = np.squeeze(\n",
    "            torch.nn.functional.sigmoid(y.logits).detach().numpy()\n",
    "        ).tolist()\n",
    "        for i, p in enumerate(probability):\n",
    "            if p < threshold:\n",
    "                continue\n",
    "            pico_class.append(model.config.id2label[i])\n",
    "            confidence.append(p)\n",
    "            spans.append(Span(start=start, length=len(content)))\n",
    "    return nms_span(spans, pico_class, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e10c72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 4, 4, 4, 4]\n",
      "[0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 2]\n",
      "Span(start=2, length=2) INTERVENTIONS 0.9953840374946594\n",
      "Span(start=5, length=2) INTERVENTIONS 0.9956023693084717\n",
      "Span(start=11, length=4) PARTICIPANTS 0.8516499996185303\n"
     ]
    }
   ],
   "source": [
    "val = ebm_nlp['validation']\n",
    "tokens, boundary_pred = val['tokens'][0], val['boundary_pred'][0]\n",
    "spans, pico_class, confidence = extract_pico_elements_util(tokens, boundary_pred, tokenizer, model)\n",
    "\n",
    "print(val['original_labels'][0])\n",
    "print(boundary_pred)\n",
    "for s, c, conf in zip(spans, pico_class, confidence):\n",
    "    print(s, c, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659f2f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comparison',\n",
       " 'of',\n",
       " 'budesonide',\n",
       " 'Turbuhaler',\n",
       " 'with',\n",
       " 'budesonide',\n",
       " 'aqua',\n",
       " 'in',\n",
       " 'the',\n",
       " 'treatment',\n",
       " 'of',\n",
       " 'seasonal',\n",
       " 'allergic',\n",
       " 'rhinitis',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357e4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pico_elements(dataset_dict, dataset_split, output_path, model, tokenizer, threshold=0.5):\n",
    "    output_file = os.path.join(output_path, '{}_pico_spans.json'.format(dataset_split.name))\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    dataset = dataset_dict[dataset_split.name]\n",
    "    progress_bar = tqdm(range(len(dataset)))\n",
    "    with open(output_file, 'w+') as fout:\n",
    "        for i in range(len(dataset)):\n",
    "            row = {}\n",
    "            row['pmid'] = dataset['pmid'][i]\n",
    "            row['tokens'] = dataset['tokens'][i]\n",
    "            row['original_labels'] = dataset['original_labels'][i]\n",
    "            row['boundary_pred'] = dataset['boundary_pred'][i]\n",
    "            start_confidence = dataset['start_confidence'][i]\n",
    "            end_confidence = dataset['end_confidence'][i]\n",
    "            spans, pico_class, confidence = extract_pico_elements_util(\n",
    "                row['tokens'], row['boundary_pred'],\n",
    "                tokenizer, model,\n",
    "                threshold=threshold)\n",
    "            row['pico_elements'] = {}\n",
    "            for s, c, conf in zip(spans, pico_class, confidence):\n",
    "                if c not in row['pico_elements']:\n",
    "                    row['pico_elements'][c] = []\n",
    "                span_dict = {}\n",
    "                span_dict['span_start'] = s.start\n",
    "                span_dict['span_length'] = s.length\n",
    "                span_dict['confidence'] = conf\n",
    "                row['pico_elements'][c].append(span_dict)\n",
    "                row['start_confidence'] = start_confidence[s.start]\n",
    "                row['end_confidence'] = end_confidence[s.start + s.length - 1]\n",
    "            fout.write('{}\\n'.format(json.dumps(row)))\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e814719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a97795d55d45f49ee9c70032bd4bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_pico_elements(ebm_nlp, DatasetSplit.test, OUTPUT_PATH, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae50dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH_t = 'data/bioc/json/step_2_span_clf/threshold'\n",
    "\n",
    "# def extract_pico_elements_exp(b_threshold, output_path, model, tokenizer, threshold=0.5):\n",
    "#     output_file = os.path.join(output_path, f'{b_threshold:.2f}_pico_spans.json')\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "        \n",
    "#     input_folder = 'data/bioc/json/step_1_boundary_pred/threshold'\n",
    "#     dataset_dict = load_dataset(\n",
    "#         'json',\n",
    "#         data_files = {\n",
    "#             'train': os.path.join(input_folder, f'{b_threshold:.2f}_boundary_pred.json'), # not used\n",
    "#             'validation': os.path.join(input_folder, f'{b_threshold:.2f}_boundary_pred.json'),\n",
    "#             'test': os.path.join(input_folder, f'{b_threshold:.2f}_boundary_pred.json')\n",
    "#         }\n",
    "#     )\n",
    "        \n",
    "#     dataset = dataset_dict['test']\n",
    "#     progress_bar = tqdm(range(len(dataset)))\n",
    "#     with open(output_file, 'w+') as fout:\n",
    "#         for i in range(len(dataset)):\n",
    "#             row = {}\n",
    "#             row['pmid'] = dataset['pmid'][i]\n",
    "#             row['tokens'] = dataset['tokens'][i]\n",
    "#             row['original_labels'] = dataset['original_labels'][i]\n",
    "#             row['boundary_pred'] = dataset['boundary_pred'][i]\n",
    "#             start_confidence = dataset['start_confidence'][i]\n",
    "#             end_confidence = dataset['end_confidence'][i]\n",
    "#             spans, pico_class, confidence = extract_pico_elements_util(\n",
    "#                 row['tokens'], row['boundary_pred'],\n",
    "#                 tokenizer, model,\n",
    "#                 threshold=threshold)\n",
    "#             row['pico_elements'] = {}\n",
    "#             for s, c, conf in zip(spans, pico_class, confidence):\n",
    "#                 if c not in row['pico_elements']:\n",
    "#                     row['pico_elements'][c] = []\n",
    "#                 span_dict = {}\n",
    "#                 span_dict['span_start'] = s.start\n",
    "#                 span_dict['span_length'] = s.length\n",
    "#                 span_dict['confidence'] = conf\n",
    "#                 row['pico_elements'][c].append(span_dict)\n",
    "#                 row['start_confidence'] = start_confidence[s.start]\n",
    "#                 row['end_confidence'] = end_confidence[s.start + s.length - 1]\n",
    "#             fout.write('{}\\n'.format(json.dumps(row)))\n",
    "#             progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/gzhang/.cache/huggingface/datasets/json/default-4501c2141bd1f5c2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e411cd9383054836ab979bddd60b8525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae525b9efe6f4c5da51cfaa0942b8c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/gzhang/.cache/huggingface/datasets/json/default-4501c2141bd1f5c2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714e690d968a40029449085a17a22ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d29f85ed81d41c3b25c13be96eeabf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract_pico_elements_exp(0.2, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55771525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_pico_elements_exp(0.25, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df791e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_pico_elements_exp(0.3, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94301485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_pico_elements_exp(0.35, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_pico_elements_exp(0.4, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38044374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_pico_elements_exp(0.45, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f778d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_pico_elements_exp(0.5, OUTPUT_PATH_t, model, tokenizer, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8987b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
