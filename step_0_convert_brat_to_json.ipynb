{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab720e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c3788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pmids(folder):\n",
    "    return [f.rstrip('.txt') for f in os.listdir(folder) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194e59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(folder, pmid):\n",
    "    annotations = []\n",
    "    with open(os.path.join(folder, f'{pmid}.ann'), 'r') as f:\n",
    "        for line in f:\n",
    "            elements = line.rstrip().split('\\t')\n",
    "            assert len(elements) == 3, f'Bad format annotation: {folder}, {pmid}, {line}'\n",
    "            tokens = elements[-1].split()\n",
    "            label = elements[1][0]\n",
    "            annotations.append((label, tokens))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25444ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(folder, pmid):\n",
    "    sentences = []\n",
    "    with open(os.path.join(folder, f'{pmid}.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.rstrip().split()\n",
    "            sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2c9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = defaultdict(lambda: 0)\n",
    "label_encoding['P'] = 4\n",
    "label_encoding['I'] = 2\n",
    "label_encoding['O'] = 1\n",
    "\n",
    "def encode_pico_labels(tokens, annotations):\n",
    "    labels = [0 for _ in tokens]\n",
    "    for annotation in annotations:\n",
    "        label, tokens_in_span = annotation\n",
    "        span_length = len(tokens_in_span)\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i: i+span_length] == tokens_in_span:\n",
    "                encoding = label_encoding[label]\n",
    "                labels[i: i+span_length] = [\n",
    "                    l | encoding for l in labels[i: i+span_length]]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95185366",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = 'data/raw/brat/AD'\n",
    "test_pmids = load_pmids(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e314a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test content: pmid: .../brat/AD/10354658\n",
    "\n",
    "Text:\n",
    "Title : Selegiline in the treatment of Alzheimer ' s disease : a long - term randomized placebo - controlled trial . \n",
    "Czech and Slovak Senile Dementia of Alzheimer Type Study Group . \n",
    "METHODS : Long - term , double - blind , placebo - controlled trial . \n",
    "Seven cities ( 1 or 2 nursing homes in each city ) in the Czech and Slovak Republics . \n",
    "A total of 173 nursing - home residents fulfilling the DSM - III criteria for mild to moderate Alzheimer ' s disease . \n",
    "Selegiline ( 10 mg per day ) or placebo ( both including 50 mg ascorbic acid ) administered for 24 weeks . \n",
    "Clinical Global Impressions scale and Nurses Observation Scale for Inpatient Evaluation at baseline and at weeks 6 , 12 and 24 ; Clock Drawing Test at baseline and 24 weeks , results of which were evaluated as normal or pathologic , and quantitatively on a modified 6 - point scale ; Sternberg ' s Memory Scanning test at baseline and at weeks 6 , 12 and 24 ; Mini Mental State Examination , and electroencephalogram at baseline and 24 weeks ; Structured Adverse Effects Rating Scale ; physical , laboratory , hematological \n",
    "and electrocardiographic examinations at baseline and weeks 12 and 24 . \n",
    "\n",
    "Annotation:\n",
    "T1\tI 8 18\tSelegiline\n",
    "T2\tP 343 459\tA total of 173 nursing - home residents fulfilling the DSM - III criteria for mild to moderate Alzheimer ' s disease\n",
    "T3\tI 463 473\tSelegiline\n",
    "T4\tC 495 502\tplacebo\n",
    "T5\tO 571 604\tClinical Global Impressions scale\n",
    "T6\tO 609 658\tNurses Observation Scale for Inpatient Evaluation\n",
    "T7\tO 700 718\tClock Drawing Test\n",
    "T8\tO 855 889\tSternberg ' s Memory Scanning test\n",
    "T9\tO 931 960\tMini Mental State Examination\n",
    "T10\tO 967 987\telectroencephalogram\n",
    "T11\tO 1015 1054\tStructured Adverse Effects Rating Scale\n",
    "T12\tO 1057 1065\tphysical\n",
    "T13\tO 1068 1078\tlaboratory\n",
    "T14\tO 1081 1094\thematological\n",
    "T15\tO 1100 1133\telectrocardiographic examinations\n",
    "\n",
    "'''\n",
    "test_pmid = test_pmids[0]\n",
    "test_annotations = load_annotations(test_folder, test_pmid)\n",
    "test_sentences = load_sentences(test_folder, test_pmid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd236d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'total',\n",
       " 'of',\n",
       " '173',\n",
       " 'nursing',\n",
       " '-',\n",
       " 'home',\n",
       " 'residents',\n",
       " 'fulfilling',\n",
       " 'the',\n",
       " 'DSM',\n",
       " '-',\n",
       " 'III',\n",
       " 'criteria',\n",
       " 'for',\n",
       " 'mild',\n",
       " 'to',\n",
       " 'moderate',\n",
       " 'Alzheimer',\n",
       " \"'\",\n",
       " 's',\n",
       " 'disease',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327c44a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_pico_labels(test_sentences[4], test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa6901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset_in_json(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    with open(os.path.join(output_folder, 'test_all.json'), 'w+') as fout:\n",
    "        pmids = load_pmids(input_folder)\n",
    "        for pmid in pmids:\n",
    "            annotations = load_annotations(input_folder, pmid)\n",
    "            sentences = load_sentences(input_folder, pmid)\n",
    "            for sentence in sentences:\n",
    "                data = {}\n",
    "                data['pmid'] = pmid\n",
    "                data['tokens'] = sentence\n",
    "                data['labels'] = encode_pico_labels(sentence, annotations)\n",
    "                fout.write('{}\\n'.format(json.dumps(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac66189",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_dataset_in_json('data/raw/brat/AD', 'data/bioc/json/brat/AD')\n",
    "format_dataset_in_json('data/raw/brat/COVID', 'data/bioc/json/brat/COVID')\n",
    "format_dataset_in_json('data/raw/brat/EBM-NLP', 'data/bioc/json/brat/EBM-NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99705e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_val_test_pmids(input_folder):\n",
    "    pmids = load_pmids(input_folder)\n",
    "    total_pmids = len(pmids)\n",
    "    pmids_train = pmids[:int(total_pmids*.45)]\n",
    "    pmids_val = pmids[int(total_pmids*.45): int(total_pmids*.5)]\n",
    "    pmids_test = pmids[int(total_pmids*.5):]\n",
    "    return pmids_train, pmids_val, pmids_test\n",
    "\n",
    "def aggregate_brat_datasets(input_folders, pmid_lists, output_fn):\n",
    "    with open(output_fn, 'w+') as fout:\n",
    "        for input_folder, pmids in zip(input_folders, pmid_lists):\n",
    "            for pmid in pmids:\n",
    "                annotations = load_annotations(input_folder, pmid)\n",
    "                sentences = load_sentences(input_folder, pmid)\n",
    "                for sentence in sentences:\n",
    "                    data = {}\n",
    "                    data['pmid'] = pmid\n",
    "                    data['tokens'] = sentence\n",
    "                    data['labels'] = encode_pico_labels(sentence, annotations)\n",
    "                    fout.write('{}\\n'.format(json.dumps(data)))\n",
    "\n",
    "def create_brat_train_and_val_set():\n",
    "    ad_folder = 'data/raw/brat/AD'\n",
    "    ad_pmids_train, ad_pmids_val, _ = load_train_val_test_pmids(ad_folder)\n",
    "    \n",
    "    covid_folder = 'data/raw/brat/COVID'\n",
    "    covid_pmids_train, covid_pmids_val, _ = load_train_val_test_pmids(covid_folder)\n",
    "    \n",
    "    ebm_nlp_folder = 'data/raw/brat/EBM-NLP'\n",
    "    ebm_nlp_pmids_train, ebm_nlp_pmids_val, _ = load_train_val_test_pmids(ebm_nlp_folder)\n",
    "    \n",
    "    train_file = 'data/bioc/json/brat/train.json'\n",
    "    val_file = 'data/bioc/json/brat/validation.json'\n",
    "    \n",
    "    aggregate_brat_datasets(\n",
    "        [ad_folder, covid_folder, ebm_nlp_folder],\n",
    "        [ad_pmids_train, covid_pmids_train, ebm_nlp_pmids_train],\n",
    "        train_file,\n",
    "    )\n",
    "    \n",
    "    aggregate_brat_datasets(\n",
    "        [ad_folder, covid_folder, ebm_nlp_folder],\n",
    "        [ad_pmids_val, covid_pmids_val, ebm_nlp_pmids_val],\n",
    "        val_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfdde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brat_test_sets_util(input_folder, pmids, output_folder):\n",
    "    with open(os.path.join(output_folder, 'test.json'), 'w+') as fout:\n",
    "        for pmid in pmids:\n",
    "            annotations = load_annotations(input_folder, pmid)\n",
    "            sentences = load_sentences(input_folder, pmid)\n",
    "            for sentence in sentences:\n",
    "                data = {}\n",
    "                data['pmid'] = pmid\n",
    "                data['tokens'] = sentence\n",
    "                data['labels'] = encode_pico_labels(sentence, annotations)\n",
    "                fout.write('{}\\n'.format(json.dumps(data)))\n",
    "\n",
    "def create_brat_test_sets():\n",
    "    ad_folder = 'data/raw/brat/AD'  \n",
    "    covid_folder = 'data/raw/brat/COVID'    \n",
    "    ebm_nlp_folder = 'data/raw/brat/EBM-NLP'\n",
    "    \n",
    "    output_dirs = [\n",
    "        'data/bioc/json/brat/AD',\n",
    "        'data/bioc/json/brat/COVID',\n",
    "        'data/bioc/json/brat/EBM-NLP',\n",
    "    ]\n",
    "    \n",
    "    for input_dir, output_dir in zip([ad_folder, covid_folder, ebm_nlp_folder], output_dirs):\n",
    "        _, _, pmids_test = load_train_val_test_pmids(input_dir)\n",
    "        create_brat_test_sets_util(input_dir, pmids_test, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c2a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_brat_train_and_val_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ccdfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_brat_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe17e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brat_ad_train_and_val_set():\n",
    "    ad_folder = 'data/raw/brat/AD'\n",
    "    ad_pmids_train, ad_pmids_val, _ = load_train_val_test_pmids(ad_folder)\n",
    "    \n",
    "    train_file = 'data/bioc/json/brat/train_ad.json'\n",
    "    val_file = 'data/bioc/json/brat/validation_ad.json'\n",
    "    \n",
    "    aggregate_brat_datasets(\n",
    "        [ad_folder],\n",
    "        [ad_pmids_train],\n",
    "        train_file,\n",
    "    )\n",
    "    \n",
    "    aggregate_brat_datasets(\n",
    "        [ad_folder],\n",
    "        [ad_pmids_val],\n",
    "        val_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0ee1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_brat_ad_train_and_val_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fecb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PicoType(Enum):\n",
    "    PARTICIPANTS = 4\n",
    "    INTERVENTIONS = 2\n",
    "    OUTCOMES = 1\n",
    "\n",
    "class Span:\n",
    "    def __init__(self, start, length):\n",
    "        self.start = start\n",
    "        self.length = length\n",
    "        self.end = start + length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Span(start:{self.start}, length:{self.length})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.start, int(self.length)) == (other.start, int(other.length))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.start, int(self.length)))\n",
    "\n",
    "def extract_pico_spans_from_encodings(label_encodings, pico_type):\n",
    "    if not label_encoding:\n",
    "        return []\n",
    "    \n",
    "    labels = [l & pico_type.value for l in label_encodings]\n",
    "    span_start = [0 for l in labels]\n",
    "    span_length = [0.0 for l in labels]\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label > 0:\n",
    "            if i==0 or labels[i-1] <= 0:\n",
    "                span_start[i] = 1\n",
    "                start = i\n",
    "            span_length[start] += 1\n",
    "            \n",
    "    spans = []\n",
    "    for i in range(len(span_start)):\n",
    "        if span_start[i]:\n",
    "            s = Span(start=i, length=span_length[i])\n",
    "            spans.append(s)\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195d4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_span_util(span_a, span_b):\n",
    "    if span_a.start > span_b.start:\n",
    "        span_a, span_b = span_b, span_a\n",
    "    spans = []\n",
    "    if span_b.start >= span_a.end:\n",
    "        spans.append(Span(span_a.start, span_b.end - span_a.start))\n",
    "    else:\n",
    "        if span_a.start != span_b.start and span_a.start != span_b.end:\n",
    "            spans.append(Span(span_b.start, span_a.end - span_b.start))\n",
    "            spans.append(Span(span_a.start, span_b.end - span_a.start))\n",
    "    return spans\n",
    "        \n",
    "\n",
    "def synthesize_spans(span_list_a, span_list_b):\n",
    "    spans = []\n",
    "    for a in span_list_a:\n",
    "        for b in span_list_b:\n",
    "            spans += synthesize_span_util(a, b)\n",
    "    return spans\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e76a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brat_span_clf_datasets_util(input_folders, pmid_lists, output_fn):\n",
    "    output_path = os.path.dirname(output_fn)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    count = defaultdict(lambda: 0)\n",
    "    with open(output_fn, 'w+') as fout:\n",
    "        for input_folder, pmids in zip(input_folders, pmid_lists):\n",
    "            for pmid in pmids:\n",
    "                annotations = load_annotations(input_folder, pmid)\n",
    "                sentences = load_sentences(input_folder, pmid)\n",
    "                for sentence in sentences:\n",
    "                    label_encoding = encode_pico_labels(sentence, annotations)\n",
    "                    for pico_type in list(PicoType):\n",
    "                        label = pico_type.name\n",
    "                        spans = extract_pico_spans_from_encodings(label_encoding, pico_type)\n",
    "                        if pico_type == PicoType.PARTICIPANTS:\n",
    "                            participants_spans = spans\n",
    "                        if pico_type == PicoType.INTERVENTIONS:\n",
    "                            interventions_spans = spans\n",
    "                        if pico_type == PicoType.OUTCOMES:\n",
    "                            outcomes_spans = spans\n",
    "                        for span in spans:\n",
    "                            data = {}\n",
    "                            data['pmid'] = pmid\n",
    "                            start, end = int(span.start), int(span.start + span.length)\n",
    "                            data['tokens'] = sentence[start:end]\n",
    "                            data['PARTICIPANTS'] = False\n",
    "                            data['INTERVENTIONS'] = False\n",
    "                            data['OUTCOMES'] = False\n",
    "                            data[pico_type.name] = True\n",
    "                            fout.write('{}\\n'.format(json.dumps(data)))\n",
    "                            count[label] += 1\n",
    "                    \n",
    "                    # synthesize spans using boundaries\n",
    "                    synthesized_spans = synthesize_spans(\n",
    "                        participants_spans, interventions_spans\n",
    "                    ) + synthesize_spans(\n",
    "                        participants_spans, outcomes_spans\n",
    "                    ) + synthesize_spans(\n",
    "                        interventions_spans, outcomes_spans\n",
    "                    )\n",
    "                    synthesized_spans = list(set(synthesized_spans))\n",
    "                    random.shuffle(synthesized_spans)\n",
    "                    sample_limit = 1\n",
    "                    for span in synthesized_spans[:sample_limit]:\n",
    "                        if (span not in participants_spans \n",
    "                            and span not in interventions_spans \n",
    "                            and span not in outcomes_spans):\n",
    "                            data = {}\n",
    "                            data['pmid'] = pmid\n",
    "                            start, end = int(span.start), int(span.start + span.length)\n",
    "                            data['tokens'] = sentence[start:end]\n",
    "                            data['PARTICIPANTS'] = False\n",
    "                            data['INTERVENTIONS'] = False\n",
    "                            data['OUTCOMES'] = False\n",
    "                            fout.write('{}\\n'.format(json.dumps(data)))\n",
    "                            count['SYNTHESIZED'] += 1\n",
    "                            fout.write('{}\\n'.format(json.dumps(data)))\n",
    "    print('{}： {}'.format(output_fn, ', '.join([f'{k}: {count[k]}' for k in count])))\n",
    "\n",
    "\n",
    "def create_brat_span_clf_datasets():\n",
    "    ad_folder = 'data/raw/brat/AD'\n",
    "    ad_pmids_train, ad_pmids_val, _ = load_train_val_test_pmids(ad_folder)\n",
    "    \n",
    "    covid_folder = 'data/raw/brat/COVID'\n",
    "    covid_pmids_train, covid_pmids_val, _ = load_train_val_test_pmids(covid_folder)\n",
    "    \n",
    "    ebm_nlp_folder = 'data/raw/brat/EBM-NLP'\n",
    "    ebm_nlp_pmids_train, ebm_nlp_pmids_val, _ = load_train_val_test_pmids(ebm_nlp_folder)\n",
    "    \n",
    "    train_file = 'data/bioc/json/brat/train_span_clf.json'\n",
    "    val_file = 'data/bioc/json/brat/validation_span_clf.json'\n",
    "    \n",
    "    create_brat_span_clf_datasets_util(\n",
    "        [ad_folder, covid_folder, ebm_nlp_folder],\n",
    "        [ad_pmids_train, covid_pmids_train, ebm_nlp_pmids_train],\n",
    "        train_file,\n",
    "    )\n",
    "    \n",
    "    create_brat_span_clf_datasets_util(\n",
    "        [ad_folder, covid_folder, ebm_nlp_folder],\n",
    "        [ad_pmids_val, covid_pmids_val, ebm_nlp_pmids_val],\n",
    "        val_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d67e96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# create_brat_span_clf_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd2255d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_brat_ad_span_clf_datasets():\n",
    "#     ad_folder = 'data/raw/brat/AD'\n",
    "#     ad_pmids_train, ad_pmids_val, _ = load_train_val_test_pmids(ad_folder)\n",
    "    \n",
    "#     train_file = 'data/bioc/json/brat/train_ad_span_clf.json'\n",
    "#     val_file = 'data/bioc/json/brat/validation_ad_span_clf.json'\n",
    "    \n",
    "#     create_brat_span_clf_datasets_util(\n",
    "#         [ad_folder],\n",
    "#         [ad_pmids_train],\n",
    "#         train_file,\n",
    "#     )\n",
    "    \n",
    "#     create_brat_span_clf_datasets_util(\n",
    "#         [ad_folder],\n",
    "#         [ad_pmids_val],\n",
    "#         val_file,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7020a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_brat_ad_span_clf_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6785af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(input_folders):\n",
    "    for input_folder in input_folders:\n",
    "        count = defaultdict(lambda: 0)\n",
    "        pmid_lists = load_train_val_test_pmids(input_folder)\n",
    "        for pmids in pmid_lists:\n",
    "            print(len(pmids))\n",
    "            for pmid in pmids:\n",
    "                annotations = load_annotations(input_folder, pmid)\n",
    "                sentences = load_sentences(input_folder, pmid)\n",
    "                for sentence in sentences:\n",
    "                    label_encoding = encode_pico_labels(sentence, annotations)\n",
    "                    for pico_type in list(PicoType):\n",
    "                        label = pico_type.name\n",
    "                        spans = extract_pico_spans_from_encodings(label_encoding, pico_type)\n",
    "                        if pico_type == PicoType.PARTICIPANTS:\n",
    "                            participants_spans = spans\n",
    "                        if pico_type == PicoType.INTERVENTIONS:\n",
    "                            interventions_spans = spans\n",
    "                        if pico_type == PicoType.OUTCOMES:\n",
    "                            outcomes_spans = spans\n",
    "                        for span in spans:\n",
    "                            data = {}\n",
    "                            data['pmid'] = pmid\n",
    "                            start, end = int(span.start), int(span.start + span.length)\n",
    "                            data['tokens'] = sentence[start:end]\n",
    "                            data['PARTICIPANTS'] = False\n",
    "                            data['INTERVENTIONS'] = False\n",
    "                            data['OUTCOMES'] = False\n",
    "                            data[pico_type.name] = True\n",
    "                            count[label] += 1\n",
    "        print('{}： {}'.format(input_folder, ', '.join([f'{k}: {count[k]}' for k in count])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c256785",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_folder = 'data/raw/brat/AD'  \n",
    "covid_folder = 'data/raw/brat/COVID'    \n",
    "ebm_nlp_folder = 'data/raw/brat/EBM-NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8008d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "8\n",
      "75\n",
      "data/raw/brat/AD： INTERVENTIONS: 490, PARTICIPANTS: 218, OUTCOMES: 656\n"
     ]
    }
   ],
   "source": [
    "count_entities([ad_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1304ff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "8\n",
      "75\n",
      "data/raw/brat/COVID： PARTICIPANTS: 263, INTERVENTIONS: 652, OUTCOMES: 619\n"
     ]
    }
   ],
   "source": [
    "count_entities([covid_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb21d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
